# PRD: Phase 8 - Integration Tests

## Introduction

Implement comprehensive integration tests that verify the entire Agent Spaces v2 system works end-to-end. Tests use a claude-shim to avoid requiring real Claude, with optional real-Claude tests gated behind an environment variable.

## Goals

- Verify all CLI commands work correctly
- Test resolution, materialization, and caching
- Test error handling and edge cases
- Use claude-shim for fast, reliable CI
- Support optional real-Claude tests for validation

## User Stories

### US-001: Create Test Fixtures Directory Structure
**Description:** As a developer, I need test fixtures to run integration tests against.

**Acceptance Criteria:**
- [ ] `integration-tests/fixtures/` directory
- [ ] `fixtures/sample-registry/` - git repo with test spaces
- [ ] `fixtures/sample-project/` - project with asp-targets.toml
- [ ] `fixtures/claude-shim/` - mock claude executable
- [ ] All fixtures committed to repo
- [ ] Fixtures documented in README

### US-002: Create Sample Registry Fixture
**Description:** As a developer, I need a sample registry for testing resolution.

**Acceptance Criteria:**
- [ ] Git repo with proper structure
- [ ] `registry/dist-tags.json` with test entries
- [ ] `spaces/base/` - minimal space with no deps
- [ ] `spaces/frontend/` - space that depends on base
- [ ] `spaces/backend/` - space with commands and hooks
- [ ] `spaces/circular-a/` and `spaces/circular-b/` - for cycle testing
- [ ] `spaces/with-mcp/` - space with MCP config
- [ ] Multiple version tags per space
- [ ] Fixture setup script to initialize git repo

### US-003: Create Sample Project Fixture
**Description:** As a developer, I need a sample project for testing project-mode commands.

**Acceptance Criteria:**
- [ ] `asp-targets.toml` with multiple targets
- [ ] Target that composes multiple spaces
- [ ] Target that uses semver range
- [ ] Target that uses dist-tag
- [ ] No asp-lock.json (generated by tests)

### US-004: Implement Claude Shim
**Description:** As a developer, I need a mock claude executable for testing.

**Acceptance Criteria:**
- [ ] Executable script at `fixtures/claude-shim/claude`
- [ ] Records all arguments to a temp file
- [ ] Validates --plugin-dir paths exist
- [ ] Validates plugin.json exists in each plugin dir
- [ ] Exits 0 on success
- [ ] Respects ASP_CLAUDE_PATH env var
- [ ] Works on macOS and Linux

### US-005: Implement Test Harness
**Description:** As a developer, I need a test harness for running asp commands.

**Acceptance Criteria:**
- [ ] `runAsp(args: string[], options?: RunOptions): Promise<RunResult>` helper
- [ ] Sets ASP_HOME to temp directory
- [ ] Sets ASP_CLAUDE_PATH to shim
- [ ] Captures stdout and stderr
- [ ] Returns exit code
- [ ] Cleans up temp directories after test
- [ ] Supports custom env vars

### US-006: Test asp install Flow
**Description:** As a developer, I need to verify asp install works correctly.

**Acceptance Criteria:**
- [ ] Test: install in project generates asp-lock.json
- [ ] Test: lock file has correct structure
- [ ] Test: lock file has correct load order
- [ ] Test: store is populated with snapshots
- [ ] Test: second install is fast (cache hit)
- [ ] Test: install with invalid target fails with clear error

### US-007: Test asp run Flow
**Description:** As a developer, I need to verify asp run invokes Claude correctly.

**Acceptance Criteria:**
- [ ] Test: run target invokes claude with --plugin-dir flags
- [ ] Test: plugin dirs are in correct load order
- [ ] Test: plugins contain valid plugin.json
- [ ] Test: MCP config is passed when present
- [ ] Test: run with invalid target fails
- [ ] Test: run in global mode works
- [ ] Verify via shim's recorded arguments

### US-008: Test asp build Flow
**Description:** As a developer, I need to verify asp build materializes correctly.

**Acceptance Criteria:**
- [ ] Test: build creates output directory
- [ ] Test: output contains one dir per space
- [ ] Test: each dir has .claude-plugin/plugin.json
- [ ] Test: components are present
- [ ] Test: build does NOT invoke claude

### US-009: Test asp explain Flow
**Description:** As a developer, I need to verify asp explain shows correct info.

**Acceptance Criteria:**
- [ ] Test: explain shows resolved versions
- [ ] Test: explain shows load order
- [ ] Test: explain --json outputs valid JSON
- [ ] Test: JSON contains expected fields

### US-010: Test Resolution Edge Cases
**Description:** As a developer, I need to verify resolution handles edge cases.

**Acceptance Criteria:**
- [ ] Test: circular dependency fails with clear error
- [ ] Test: error message includes cycle path
- [ ] Test: diamond dependency resolves correctly
- [ ] Test: missing space fails with clear error
- [ ] Test: invalid semver range fails
- [ ] Test: unknown dist-tag fails

### US-011: Test asp add/remove/upgrade
**Description:** As a developer, I need to verify target modification commands.

**Acceptance Criteria:**
- [ ] Test: add updates asp-targets.toml correctly
- [ ] Test: add runs install after update
- [ ] Test: remove updates asp-targets.toml correctly
- [ ] Test: remove fails if space not in target
- [ ] Test: upgrade updates lock file
- [ ] Test: upgrade respects semver ranges

### US-012: Test asp diff
**Description:** As a developer, I need to verify diff shows correct changes.

**Acceptance Criteria:**
- [ ] Test: diff shows added spaces
- [ ] Test: diff shows removed spaces
- [ ] Test: diff shows upgraded spaces
- [ ] Test: diff does not modify files
- [ ] Test: diff --json outputs valid JSON

### US-013: Test asp lint
**Description:** As a developer, I need to verify lint detects issues.

**Acceptance Criteria:**
- [ ] Test: lint detects command name collision (W201)
- [ ] Test: lint detects missing plugin root in hooks (W203)
- [ ] Test: lint detects invalid hooks config (W204)
- [ ] Test: lint detects plugin name collision (W205)
- [ ] Test: lint passes on valid project
- [ ] Test: lint --json outputs valid JSON

### US-014: Test asp repo Commands
**Description:** As a developer, I need to verify repo management commands.

**Acceptance Criteria:**
- [ ] Test: repo init creates registry structure
- [ ] Test: repo init creates dist-tags.json
- [ ] Test: repo publish creates git tag
- [ ] Test: repo publish updates dist-tags.json
- [ ] Test: repo tags lists versions correctly
- [ ] Test: repo status shows correct info

### US-015: Test Cache Behavior
**Description:** As a developer, I need to verify caching works correctly.

**Acceptance Criteria:**
- [ ] Test: first materialization populates cache
- [ ] Test: second materialization uses cache (fast)
- [ ] Test: cache key changes when space content changes
- [ ] Test: gc removes unreferenced entries
- [ ] Test: gc preserves referenced entries

### US-016: Test Real Claude (Optional)
**Description:** As a developer, I need optional tests with real Claude for validation.

**Acceptance Criteria:**
- [ ] Tests in `run-real-claude.test.ts`
- [ ] Skipped by default
- [ ] Enabled with RUN_REAL_CLAUDE=1 env var
- [ ] Test: Claude boots with plugins
- [ ] Test: /plugin:command is available
- [ ] Test: MCP servers connect (if configured)
- [ ] Tests must be idempotent

### US-017: Implement CI Configuration
**Description:** As a developer, I need CI to run integration tests.

**Acceptance Criteria:**
- [ ] GitHub Actions workflow file
- [ ] Runs on push and PR
- [ ] Sets up Bun
- [ ] Runs unit tests
- [ ] Runs integration tests with shim
- [ ] Caches dependencies
- [ ] Reports test results

## Functional Requirements

- FR-1: All tests must be deterministic (no flaky tests)
- FR-2: Tests must clean up after themselves
- FR-3: Tests must not require network access (except real-claude tests)
- FR-4: Test fixtures must be committed and version-controlled
- FR-5: Tests must run in < 60 seconds total

## Non-Goals

- No performance benchmarking
- No load testing
- No cross-platform CI (Linux only for now)

## Technical Considerations

- Use Bun's built-in test runner
- Create fresh temp directories for each test
- Git fixtures need to be real git repos (can't fake .git)
- Claude shim must handle all expected arguments

## Success Metrics

- All integration tests pass
- Tests complete in < 60 seconds
- No flaky tests
- Coverage of all CLI commands
- Edge cases documented and tested

## Open Questions

- Should we add snapshot testing for CLI output?
- Should we test Windows compatibility?
